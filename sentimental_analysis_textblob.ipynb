{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file serves to perform textblob sentiment analysis on the tweets, it was used in google colab to perform the analysis.\n",
    "\n",
    "All tweets were in parquet dataframes stored on google drive(https://drive.google.com/drive/folders/18R9D_GcczG0cFTXnDdo-Ejy2ZuyhobKA?usp=sharing)\n",
    "\n",
    "Files from original dataset were ungziped, csv files were read and then converted to parquet files in order to mantain better performance.\n",
    "I used bulks of 10 dataframes to mantain a good performance and not to overload the memory.\n",
    "\n",
    "Neutral values were not skipped, because on early stage they served no purpose, there were plenty of them, I suspect that this is due to the Textblob model characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags_links_and_emojis(text):\n",
    "    # Regular expression to match hashtags and links\n",
    "    hashtag_pattern = r'#\\S+'\n",
    "    link_pattern = r'http\\S+|www.\\S+'\n",
    "    \n",
    "    # Unicode ranges for emojis\n",
    "    emoji_pattern = r'[' \\\n",
    "                    u'\\U0001F600-\\U0001F64F' \\\n",
    "                    u'\\U0001F300-\\U0001F5FF' \\\n",
    "                    u'\\U0001F680-\\U0001F6FF' \\\n",
    "                    u'\\U0001F1E0-\\U0001F1FF' \\\n",
    "                    u'\\U00002702-\\U000027B0' \\\n",
    "                    u'\\U000024C2-\\U0001F251' \\\n",
    "                    ']+'\n",
    "    \n",
    "    # Combine all patterns\n",
    "    combined_pattern = f'({hashtag_pattern})|({link_pattern})|({emoji_pattern})'\n",
    "    \n",
    "    # Replace hashtags, links, and emojis with an empty string\n",
    "    return re.sub(combined_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_data_sets = []\n",
    "\n",
    "directory_to_dataset = './drive/MyDrive/Sentimental_Analysis/Ukraine_War_Data/UkraineWar'\n",
    "for file in os.listdir(directory_to_dataset):\n",
    "    if file.endswith('.parquet'):\n",
    "        list_of_data_sets.append(file)\n",
    "\n",
    "#sort list_of_data_sets\n",
    "list_of_data_sets.sort()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = len(list_of_data_sets)\n",
    "print(list_of_data_sets)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "tqdm.pandas() # for progress_apply, lack of it causes error\n",
    "\n",
    "k = 10 #number of datasets in one bulk\n",
    "i = 0\n",
    "\n",
    "for dataset in list_of_data_sets:\n",
    "    i += 1\n",
    "    data_set_dir = directory_to_dataset + \"/\" + dataset\n",
    "\n",
    "    # Choosing only what we need from the dataset\n",
    "    df_to_add = pd.read_parquet(data_set_dir, columns=[\"text\", \"extractedts\", \"language\"])\n",
    "    df_to_add = df_to_add[df_to_add[\"language\"] == \"en\"]\n",
    "    df_to_add = df_to_add[[\"text\", \"extractedts\"]]\n",
    "    df_to_add.dropna(inplace=True)\n",
    "    df_to_add.loc[:,\"text\"] = df_to_add[\"text\"].apply(lambda x: remove_hashtags_links_and_emojis(x)) # removing hashtags and links from texts\n",
    "\n",
    "    df = pd.concat([df, df_to_add])\n",
    "    print(f\"Read {i} of {n} datasets, {round(i/n*100, 2)}%\")\n",
    "\n",
    "\n",
    "    if i % k == 0 or i == n:\n",
    "        df.loc[:, \"polarity\"] = df[\"text\"].progress_apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "        df.loc[:, \"subjectivity\"] = df[\"text\"].progress_apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "        #remove neutral values\n",
    "        df = df[df[\"polarity\"] != 0]\n",
    "        df = df[df[\"subjectivity\"] != 0]\n",
    "        df = df[[\"polarity\", \"subjectivity\", \"extractedts\"]]\n",
    "\n",
    "        df.to_parquet(f\"./drive/MyDrive/Sentimental_Analysis/biggerdata_processed_{i}_iteration.parquet\")\n",
    "        # Clear the dataframe\n",
    "        df = pd.DataFrame()\n",
    "        print(f\"Processed {i} of {n} datasets\")\n",
    "\n",
    "print(\"DONEDONEDONEDONEDONE\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
